{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory assignment 1\n",
    "\n",
    "\n",
    "\n",
    "IN5400 / IN9400 - Machine Learning for Image Analysis<br>\n",
    "University of Oslo<br>\n",
    "Spring 2020<br>\n",
    "  \n",
    "  \n",
    "Handout: 2020.02.10<br>\n",
    "Delivery deadline: 2020.03.02\n",
    "\n",
    "## Part 1: Image classification with deep learning and dense neural networks\n",
    "\n",
    "In this exercise, you are supposed to implement a basic neural network for image classification. The network will be densly connected, with an arbitrary number of layers, and number of nodes in each layer. We shall implement a ReLu activation function, and use a softmax activation in the last layer. The error will be measured with a cross-entropy loss function, and the cost will be minimized using a stochastic gradient descent optimization routine.\n",
    "\n",
    "\n",
    "### Evaluation format\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Each subtask will be judged independently, so it should be possible to do one task even if you have not succeded in the previous. Note, however, that each step needs to be correct in order for the whole method to work at the end.\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "- All subtasks that you are to answer is found in this notebook.\n",
    "- In addition, there is a `dnn` folder that contains a skeleton for the classifier, including a complete way to import data.\n",
    "- All implementation should be done in the respective files in the `dnn` folder\n",
    "\n",
    "```\n",
    "def implement_this_function(argument_1):\n",
    "    \"\"\"This is an illustrative dummy function\"\"\"\n",
    "    # TODO: Task X.Y\n",
    "    result = None\n",
    "    \n",
    "    return result\n",
    "```\n",
    "- Some function headers are already given, and necessary, as they are called by the subtasks in this notebook.\n",
    "- Everything else you feel you need to implement, you can implement as you like.\n",
    "- When you have implemented everything (correctly), you should be able to run the whole classifier as `python dnn/main.py`\n",
    "- Suggestion about the values of different hyperparameters will be given, but you are encouraged to experiment in the final subtask.\n",
    "\n",
    "### What you should implement\n",
    "\n",
    "The skeleton of this program that is already implemented contains things such as:\n",
    "- Program setup\n",
    "- Configurations\n",
    "- Data import of three datasets: mnist, cifar10, and svhn\n",
    "- Training framework\n",
    "- Evaluation framework\n",
    "\n",
    "You should implement the content in the training framework. All steps will be given as tasks and subtasks below. The following are *you* supposed to implement.\n",
    "1. Parameter initialization\n",
    "2. Forward propagation through a network with *arbitrary number of layer* where each layer has an *arbitrary number of nodes*\n",
    "  1. ReLu activation function\n",
    "  2. Softmax function\n",
    "  3. The rest of the forward propagation\n",
    "3. Cross Entropy cost function\n",
    "4. Backward propagation through network with *arbitrary number of layer* where each layer has an *arbitrary number of nodes*\n",
    "  1. Derivative of the ReLu activation function\n",
    "  2. The rest of the backward propagation\n",
    "5. Parameter update using Gradient Descent optimization\n",
    "6. Run the finished method\n",
    "  1. Reproduce result with default settings\n",
    "  2. Exceed the default result by experimenting with different hyperparameter configurations.\n",
    "\n",
    "\n",
    "### Additional notes\n",
    "\n",
    "Most variables should be self-explanatory, but there are four important dictionaries worth mentioning, as they will control the data flow of the entire program\n",
    "\n",
    "- `conf`: Contains all configurations of the program. These configurations will be passed around most functions, even though we most often will only need a couple of them; this is so that you are freer to experiment outside the boundaries of the program skeleton. They are set with some default values in the `config()` function in `dnn/main.py`.\n",
    "- `params`: Contains all trainiable parameters, that is, all weight and bias arrays.\n",
    "- `grads`: Contains the gradients of the respective trainable parameters.\n",
    "- `features`: Contains input and output data, in addition to linear combination arrays `Z` and activation arrays `A`.\n",
    "\n",
    "It is *strongly* encouraged to implement the vectorized version of things, otherwise, things are to slow.\n",
    "\n",
    "This should be it. Let us begin.\n",
    "\n",
    "\n",
    "### Task 1.1: Parameter initialization\n",
    "\n",
    "We will see in the lecture on training neural networks that the way we initialize the weights will be important for efficient training. The weights should be initialized to small, different random numbers drawn from a Gaussian distribution with zero mean and given variance. The numbers should also be scaled. The scaling factor depends on the activation function used. With ReLU activations, we use He initialization as given below.  \n",
    "\n",
    "The function you are to implement is `initialization(layer_dimensions)`, located in `dnn/model.py`. The parameters shall have the following shape\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    W^{[l]} &\\in \\mathbb{R}^{n^{[l-1]}\\times n^{[l]}} \\\\\n",
    "    b^{[l]} &\\in \\mathbb{R}^{n^{[l]}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and have the following values\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    W_{jk}^{[l]} &\\sim \\mathcal{N}\\left(0, \\frac{2}{n^{[l-1]}}\\right) \\\\\n",
    "    b_k^{[l]} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for all $j = 1, \\ldots, n^{[l-1]}$, $k = 1, \\ldots, n^{[l]}$, $l = 1, \\ldots, L$. Here $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ means that $x$ is sampled from a normal (or gaussian) distribution with mean $\\mu$ and variance $\\sigma^2$. In order to achieve the normal sampling in python, you can use the `numpy.random.normal()` function.  \n",
    "\n",
    "Note that $b^{[l]}$ should have dimension $[n_l,1]$, not just $[n_l]$.\n",
    "\n",
    "This initialization fits well with ReLu activations, and is proposed in [He et al. (2015)](https://arxiv.org/pdf/1502.01852.pdf). For another common initialization scheme, you can study the paper by [Glorot and Hinton (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "\n",
    "The purpose of this initialization will be to let the weights in all layers be initialized with random numbers with equal variance for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Project: Predicting Strain Using Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
